# Load the Pima Indians diabetes dataset from CSV URL
import numpy as np
import pandas as pd
from sklearn import metrics
from keras.models import Model
from sklearn import decomposition
from sklearn import preprocessing
from keras import regularizers
from keras.layers import Input, Dense
from keras.models import Sequential
from sklearn_pandas import DataFrameMapper
import tensorflow as tf
tf.python.control_flow_ops = tf


global complete_frame
global train_frame
global validate_frame
global test_frame
global train_array
global test_array
global validation_array


def main():
    global complete_frame
    global train_frame
    global validate_frame
    global test_frame
    global train_array
    global test_array
    global validation_array

    complete_data = "/home/wso2123/My Work/Datasets/Breast cancer wisconsin/data.csv"
    train_data = "/home/wso2123/My Work/Datasets/Breast cancer wisconsin/uncorrected_train.csv"
    validate_data = "/home/wso2123/My Work/Datasets/Breast cancer wisconsin/validate.csv"
    test_data = "/home/wso2123/My Work/Datasets/Breast cancer wisconsin/test.csv"

    # load the CSV file as a numpy matrix
    complete_frame = pd.read_csv(complete_data)
    train_frame = pd.read_csv(train_data)
    validate_frame = pd.read_csv(validate_data)
    test_frame = pd.read_csv(test_data)

    train_frame = pd.get_dummies(train_frame)
    train_frame = train_frame.drop('diagnosis_M', axis=1)
    feature_list = list(train_frame.columns)
    print feature_list
    mapper = DataFrameMapper([(feature_list, [preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0), preprocessing.Normalizer()])])
    train_array = mapper.fit_transform(train_frame)

    test_frame = pd.get_dummies(test_frame)
    test_frame = test_frame.drop('diagnosis_M', axis=1)
    feature_list = list(test_frame.columns)
    print feature_list
    mapper = DataFrameMapper([(feature_list, [preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0), preprocessing.Normalizer()])])
    test_array = mapper.fit_transform(test_frame)

    validate_frame = pd.get_dummies(validate_frame)
    feature_list = list(validate_frame.columns)
    print feature_list
    mapper = DataFrameMapper([(feature_list, [preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0), preprocessing.Normalizer()])])
    validation_array = mapper.fit_transform(validate_frame)

    for i in range(1):
        print i, "---------------"
        model_build(12)


def model_build(i):
    # create model
    model = Sequential()
    model.add(Dense(12, input_dim=32, init='uniform', activation='relu' , activity_regularizer=regularizers.activity_l1(10e-5)))
    model.add(Dense(32, init='uniform', activation='relu'))
    # Compile model
    model.compile(optimizer='adam', loss='mean_squared_error')
    # Fit the model
    model.fit(train_array, train_array, nb_epoch=150, batch_size=20, validation_data=(validation_array, validation_array))
    # evaluate the model
    decoded_output = model.predict(test_array)

    print test_array[0]
    print decoded_output[0]

    recons_err = []
    for i in range(len(test_array)):
        recons_err.append(metrics.mean_squared_error(test_array[i], decoded_output[i]))

    tp = 0
    fp = 0
    tn = 0
    fn = 0
    lbl_list = test_frame["diagnosis_B"]
    quntile = 0.65

    threshold = get_percentile_threshold(quntile, recons_err)
    for i in range(len(recons_err)):
        if recons_err[i] > threshold:
            if lbl_list[i] == 1:
                fp += 1
            else:
                tp += 1
        else:
            if lbl_list[i] == 1:
                tn += 1
            else:
                fn += 1

    recall = 100 * float(tp) / (tp + fn)
    print "Threshold :", threshold
    print "TP :", tp, "/n"
    print "FP :", fp, "/n"
    print "TN :", tn, "/n"
    print "FN :", fn, "/n"
    print "Recall (sensitivity) true positive rate (TP / (TP + FN)) :", recall
    print "Precision (TP / (TP + FP) :", 100 * float(tp) / (tp + fp)
    print "F1 score (harmonic mean of precision and recall (sensitivity)) (2TP / (2TP + FP + FN)) :", 200 * float(tp) / (2 * tp + fp + fn)


def get_percentile_threshold(quntile, data_frame):
    var = np.array(data_frame)  # input array
    return np.percentile(var, quntile*100)

if __name__ == '__main__':
    main()